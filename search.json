[
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "parse_error_message\n\n parse_error_message (response)\n\nParse error message for raising exceptions\n\n\n\n\nDetails\n\n\n\n\nresponse\nResponse object from requests\n\n\n\n\n\n\nparse_response\n\n parse_response (response)\n\nParse response object into JSON\n\n\n\n\nDetails\n\n\n\n\nresponse\nResponse object\n\n\n\n\n\n\nIPFSGateway\n\n IPFSGateway (url)\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "estuaryapi.html#standard-ipfs-pinning-api",
    "href": "estuaryapi.html#standard-ipfs-pinning-api",
    "title": "Estuary",
    "section": "Standard IPFS Pinning API",
    "text": "Standard IPFS Pinning API\n\nPinning to/interacting with IPFS through Estuary\n\n\n\nlist_pins\n\n list_pins (api_key:str)\n\nList all your pins\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\n\n\n\n\nadd_pin\n\n add_pin (api_key:str, file_name:str, cid:str)\n\nAdd a new pin object for the current access token.\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\nfile_name\nstr\nFile name to pin\n\n\ncid\nstr\nCID to attach\n\n\n\n\n\n\nget_pin\n\n get_pin (api_key:str, pin_id:str)\n\nGet a pinned object by ID\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\npin_id\nstr\nUnique pin ID\n\n\n\n\n\n\nreplace_pin\n\n replace_pin (api_key:str, pin_id:str)\n\nReplace a pinned object by ID\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\npin_id\nstr\nUnique pin ID\n\n\n\n\n\n\nremove_pin\n\n remove_pin (api_key:str, pin_id:str)\n\nRemove a pinned object by ID\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\npin_id\nstr\nUnique pin ID\n\n\n\n\n\nCollections\n\n\ncreate_coll\n\n create_coll (api_key:str, name:str, description:str)\n\nCreate new collection\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\nname\nstr\nCollection name\n\n\ndescription\nstr\nCollection description\n\n\n\n\n\n\nadd_content\n\n add_content (api_key:str, collection_id:str, data:list, cids:list)\n\nAdd data to Collection\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\ncollection_id\nstr\nCollection ID\n\n\ndata\nlist\nList of paths to data to be added\n\n\ncids\nlist\nList of respective CIDs\n\n\n\n\n\n\nlist_colls\n\n list_colls (api_key:str)\n\nList your collections\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\n\n\n\n\nlist_coll_content\n\n list_coll_content (api_key:str, collection_id:str)\n\nList contents of a collection from ID\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\ncollection_id\nstr\nCollection ID\n\n\n\n\n\n\nlist_content_path\n\n list_content_path (api_key:str, collection_id:str, path:str)\n\nList content of a path in collection\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\ncollection_id\nstr\nCollection ID\n\n\npath\nstr\nPath in collection to list files from\n\n\n\n\n\n\nadd_content_path\n\n add_content_path (api_key:str, collection_id:str, path:str)\n\nAdd content to a specific file system path in an IPFS collection\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\ncollection_id\nstr\nCollection ID\n\n\npath\nstr\nPath in collection to add files to\n\n\n\n\n\n\nEstuary base API\n\n\nadd_key\n\n add_key (api_key, expiry='24h')\n\nAdd client safe upload key\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\napi_key\n\n\nYour Estuary API key\n\n\nexpiry\nstr\n24h\nExpiry of upload key\n\n\n\n\n\n\nadd_data\n\n add_data (api_key:str, path_to_file:str)\n\nUpload file to Estuary\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\npath_to_file\nstr\nPath to file you want to upload\n\n\n\n\n\n\nadd_cid\n\n add_cid (api_key:str, file_name:str, cid:str)\n\nUse an existing IPFS CID to make storage deals.\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\nfile_name\nstr\nFile name to add to CID\n\n\ncid\nstr\nCID for file\n\n\n\n\n\n\nadd_car\n\n add_car (api_key:str, path_to_file:str)\n\nWrite a Content-Addressable Archive (CAR) file, and make storage deals for its contents.\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\npath_to_file\nstr\nPath to file to store\n\n\n\n\n\n\nmake_deal\n\n make_deal (api_key:str, content_id:str, provider_id:str)\n\nMake a deal with a storage provider and a file you have already uploaded to Estuary\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\ncontent_id\nstr\nContent ID on Estuary\n\n\nprovider_id\nstr\nProvider ID\n\n\n\n\n\n\nview_data_cid\n\n view_data_cid (api_key:str, cid:str)\n\nView CID information\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\ncid\nstr\nCID\n\n\n\n\n\n\nlist_data\n\n list_data (api_key:str)\n\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\n\n\n\n\nlist_deals\n\n list_deals (api_key:str)\n\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\n\n\n\n\nget_deal_status\n\n get_deal_status (api_key:str, deal_id:str)\n\nGet deal status by id\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\ndeal_id\nstr\nDeal ID\n\n\n\n\n\n\nget_node_stats\n\n get_node_stats ()\n\nGet Estuary node stats\n\n\n\nget_deal_data\n\n get_deal_data ()\n\nGet on-chain deal data\n\n\n\nget_miner_ask\n\n get_miner_ask (miner_id:str)\n\nGet the query ask and verified ask for any miner\n\n\n\n\nType\nDetails\n\n\n\n\nminer_id\nstr\nMiner ID\n\n\n\n\n\n\nget_failure_logs\n\n get_failure_logs (miner_id:str)\n\nGet all of the failure logs for a specific miner\n\n\n\n\nType\nDetails\n\n\n\n\nminer_id\nstr\nMiner ID\n\n\n\n\n\n\nget_deal_logs\n\n get_deal_logs (provider_id:str)\n\nGet deal logs by provider\n\n\n\n\nType\nDetails\n\n\n\n\nprovider_id\nstr\nProvider ID\n\n\n\n\n\n\nget_provider_stats\n\n get_provider_stats (provider_id:str)\n\nGet provider stats\n\n\n\n\nType\nDetails\n\n\n\n\nprovider_id\nstr\nProvider ID\n\n\n\n\n\n\nlist_providers\n\n list_providers ()\n\nList Estuary providers\n\n\n\nDownloading data\n\n\nget_data\n\n get_data (cid:str, path_name:str)\n\nDownload data from Estuary CID\n\n\n\n\nType\nDetails\n\n\n\n\ncid\nstr\nData CID\n\n\npath_name\nstr\nPath and filename to store the file at"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to IPFSPy by Algovera",
    "section": "",
    "text": "IPFSPy is a python library by Algovera to interact with IPFS and IPFS ecosystem such as the common pinning services. It is designed by data scientists for data scientists to interact with the IPFS ecosystem without leaving the comfort of python and jupyter notebook.\nYou can learn more about IPFS here.\nIPFS is built using the go-lang or javascript. With IPFSPy, you can interact with IPFS using the exposed HTTP RPC API.\nWith IPFSPy, you can either use local, infura or public nodes. In order to use local node, you will need a IPFS deamon running in the form of IPFS Desktop, IPFS Campanion on IPFS CLI. As an alternative, you can connect via the Infura’s dedicated IPFS gateway."
  },
  {
    "objectID": "index.html#installing",
    "href": "index.html#installing",
    "title": "Welcome to IPFSPy by Algovera",
    "section": "Installing",
    "text": "Installing\nto do: instructions on how to install library goes here"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "Welcome to IPFSPy by Algovera",
    "section": "How to use",
    "text": "How to use\nTo adda file to IPFS, simply\nfrom ipfspy.ipfshttpapi import IPFSApi\n\napi = IPFSApi()\nresponse, json = api.add_items(url, \"path/to/file\")"
  },
  {
    "objectID": "ipfshttpapi.html#choosing-your-gateway",
    "href": "ipfshttpapi.html#choosing-your-gateway",
    "title": "IPFS HTTP API",
    "section": "Choosing your gateway",
    "text": "Choosing your gateway\nBy default, the IPFSApi uses the local node.\n\napi = IPFSApi()\n\nChanged to local node\n\n\nTo change to infura or public use the change_gateway_type method\n\napi.change_gateway_type = 'infura'\n\nChanged to infura node\n\n\n\napi.change_gateway_type = 'public'\n\nChanged to public node\n\n\n\napi.change_gateway_type = 'local'\n\nChanged to local node"
  },
  {
    "objectID": "ipfshttpapi.html#unixfs",
    "href": "ipfshttpapi.html#unixfs",
    "title": "IPFS HTTP API",
    "section": "UnixFS",
    "text": "UnixFS\n\nadd_items\n\nonly available on local and infura nodes\n\n\n\nIPFSApi.add_items\n\n IPFSApi.add_items (filepath:Union[str,List[str]], directory:bool=False,\n                    wrap_with_directory:bool=False,\n                    chunker:str='size-262144', pin:bool=True,\n                    hash_:str='sha2-256', progress:str='true',\n                    silent:str='false', cid_version:int=0, **kwargs)\n\nadd file/directory to ipfs\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfilepath\nUnion\n\nPath to the file/directory to be added to IPFS\n\n\ndirectory\nbool\nFalse\nIs filepath a directory\n\n\nwrap_with_directory\nbool\nFalse\nTrue if path is a directory\n\n\nchunker\nstr\nsize-262144\nChunking algorithm, size-[bytes], rabin-[min]-[avg]-[max] or buzhash\n\n\npin\nbool\nTrue\nPin this object when adding\n\n\nhash_\nstr\nsha2-256\nHash function to use. Implies CIDv1 if not sha2-256\n\n\nprogress\nstr\ntrue\nStream progress data\n\n\nsilent\nstr\nfalse\nWrite no output\n\n\ncid_version\nint\n0\nCID version\n\n\nkwargs\n\n\n\n\n\n\n\n\nUsing local IPFS Node\n\nImportant: This requires a local IPFS Node to be run. Set up your local node using info here.\n\n\nNote: Full list of available params on add function can be found here\n\nUpload a file\n\nresponse, jsonobject = api.add_items(filepath=\"../README.md\"); jsonobject\n\n[{'Name': 'README.md', 'Bytes': 1179},\n {'Name': 'README.md',\n  'Hash': 'QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B',\n  'Size': '1190'}]\n\n\nUpload multiple files\n\nresponse, jsonobject = api.add_items(filepath=[\"../README.md\", 'output/test.txt']); jsonobject\n\n[{'Name': 'README.md', 'Bytes': 1179},\n {'Name': 'README.md',\n  'Hash': 'QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B',\n  'Size': '1190'},\n {'Name': 'test.txt', 'Bytes': 24},\n {'Name': 'test.txt',\n  'Hash': 'QmTT8vwdbnP9Ls8bSY1LMyW4a8bEwTYZa5izEoJMBtTPfb',\n  'Size': '32'}]\n\n\nYou can view the added file on ipfs.io\nTo view any CID on ipfs.io https://ipfs.io/ipfs/CID\nUpload multiple file wrapped in a directory\n\nresponse, jsonobject = api.add_items(filepath=[\"../README.md\", 'output/test.txt'], wrap_with_directory='true'); jsonobject\n\n[{'Name': 'README.md', 'Bytes': 1179},\n {'Name': 'README.md',\n  'Hash': 'QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B',\n  'Size': '1190'},\n {'Name': 'test.txt', 'Bytes': 24},\n {'Name': 'test.txt',\n  'Hash': 'QmTT8vwdbnP9Ls8bSY1LMyW4a8bEwTYZa5izEoJMBtTPfb',\n  'Size': '32'},\n {'Name': '',\n  'Hash': 'Qmbu99JbYDXGCf9bprV355A5hLLE6ogHe5FM4BVAZmqU9i',\n  'Size': '1328'}]\n\n\nUpload a directory\n\nresponse, jsonobject = api.add_items(filepath='output', directory=True); jsonobject[-4:]\n\n[{'Name': 'output/fol1/fol2/test3.txt',\n  'Hash': 'Qmb6W6nVPYd5CJKFpC1zGGuoD7TYQLE5PGG1RHkHm2W3m9',\n  'Size': '40'},\n {'Name': 'output/fol1/fol2',\n  'Hash': 'QmRGeFLyf3ZANQFVdVvo8RzSrBcs7BGGqN6wn764xmwopA',\n  'Size': '3975794'},\n {'Name': 'output/fol1',\n  'Hash': 'QmdK9agin1q8YdTaUQ6v81pWBu8TencfijyPGVZ5epFxvR',\n  'Size': '7951637'},\n {'Name': 'output',\n  'Hash': 'QmWJvSt7QGY2yhr9mrfAm76vRvxXA5ygB4zd2SmfD43vtX',\n  'Size': '11927480'}]\n\n\n\n\nUsing infura.io gateway\nThis works without a local IPFS node running\n\napi.change_gateway_type = 'infura'\n\nChanged to infura node\n\n\n\nresponse, jsonobject = api.add_items(\"../README.md\"); jsonobject\n\n[{'Name': 'README.md', 'Bytes': 1179},\n {'Name': 'README.md',\n  'Hash': 'QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B',\n  'Size': '1190'}]\n\n\n\nresponse, jsonobject = api.add_items(filepath='output', directory=True); jsonobject[-4:]\n\n[{'Name': 'output/fol1/fol2/test3.txt',\n  'Hash': 'Qmb6W6nVPYd5CJKFpC1zGGuoD7TYQLE5PGG1RHkHm2W3m9',\n  'Size': '40'},\n {'Name': 'output/fol1/fol2',\n  'Hash': 'QmRGeFLyf3ZANQFVdVvo8RzSrBcs7BGGqN6wn764xmwopA',\n  'Size': '3975794'},\n {'Name': 'output/fol1',\n  'Hash': 'QmdK9agin1q8YdTaUQ6v81pWBu8TencfijyPGVZ5epFxvR',\n  'Size': '7951637'},\n {'Name': 'output',\n  'Hash': 'QmWJvSt7QGY2yhr9mrfAm76vRvxXA5ygB4zd2SmfD43vtX',\n  'Size': '11927480'}]\n\n\nYou can view the added file on infura.io\nTo view any CID on infura https://ipfs.infura.io/ipfs/CID\n\n\n\nls_items\n\nonly available on local and public nodes. ls not implemented on infura\n\n\n\nIPFSApi.ls_items\n\n IPFSApi.ls_items (cid:str, resolve_type:bool=True, size:bool=True,\n                   **kwargs)\n\nList directory contents for Unix filesystem objects\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncid\nstr\n\nThe path to the IPFS object(s) to list links from\n\n\nresolve_type\nbool\nTrue\nResolve linked objects to find out their types\n\n\nsize\nbool\nTrue\nResolve linked objects to find out their file size\n\n\nkwargs\n\n\n\n\n\n\n\napi.change_gateway_type = 'local'\n\nChanged to local node\n\n\n\nresponse, content = api.ls_items('QmeyTiqrD6oo4eQXbAt7hZn3ASzpcP3Kvb1rc9qznEowEw'); content\n\n[{'Objects': [{'Hash': 'QmeyTiqrD6oo4eQXbAt7hZn3ASzpcP3Kvb1rc9qznEowEw',\n    'Links': [{'Name': 'adult_data.csv',\n      'Hash': 'QmZnxARhJWsCbTxiAzoRhnxHgMtoEkNJNS8DGLCBEMvm4V',\n      'Size': 3974475,\n      'Type': 2,\n      'Target': ''},\n     {'Name': 'fol1',\n      'Hash': 'QmUNLLsPACCz1vLxQVkXqqLX5R1X345qqfHbsf67hvA3Nn',\n      'Size': 0,\n      'Type': 1,\n      'Target': ''},\n     {'Name': 'test.txt',\n      'Hash': 'QmTT8vwdbnP9Ls8bSY1LMyW4a8bEwTYZa5izEoJMBtTPfb',\n      'Size': 24,\n      'Type': 2,\n      'Target': ''},\n     {'Name': 'test2.txt',\n      'Hash': 'QmWa7aPQWCkLAmV2pt1Wjj3uE7V3CG3KYMKBKsVAvWG649',\n      'Size': 23,\n      'Type': 2,\n      'Target': ''},\n     {'Name': 'test3.txt',\n      'Hash': 'Qmb6W6nVPYd5CJKFpC1zGGuoD7TYQLE5PGG1RHkHm2W3m9',\n      'Size': 32,\n      'Type': 2,\n      'Target': ''}]}]}]\n\n\n\napi.change_gateway_type = 'public'\n\nChanged to public node\n\n\n\nresponse, content = api.ls_items('QmeyTiqrD6oo4eQXbAt7hZn3ASzpcP3Kvb1rc9qznEowEw'); content\n\n[{'Objects': [{'Hash': 'QmeyTiqrD6oo4eQXbAt7hZn3ASzpcP3Kvb1rc9qznEowEw',\n    'Links': [{'Name': 'adult_data.csv',\n      'Hash': 'QmZnxARhJWsCbTxiAzoRhnxHgMtoEkNJNS8DGLCBEMvm4V',\n      'Size': 3974475,\n      'Type': 2,\n      'Target': ''},\n     {'Name': 'fol1',\n      'Hash': 'QmUNLLsPACCz1vLxQVkXqqLX5R1X345qqfHbsf67hvA3Nn',\n      'Size': 0,\n      'Type': 1,\n      'Target': ''},\n     {'Name': 'test.txt',\n      'Hash': 'QmTT8vwdbnP9Ls8bSY1LMyW4a8bEwTYZa5izEoJMBtTPfb',\n      'Size': 24,\n      'Type': 2,\n      'Target': ''},\n     {'Name': 'test2.txt',\n      'Hash': 'QmWa7aPQWCkLAmV2pt1Wjj3uE7V3CG3KYMKBKsVAvWG649',\n      'Size': 23,\n      'Type': 2,\n      'Target': ''},\n     {'Name': 'test3.txt',\n      'Hash': 'Qmb6W6nVPYd5CJKFpC1zGGuoD7TYQLE5PGG1RHkHm2W3m9',\n      'Size': 32,\n      'Type': 2,\n      'Target': ''}]}]}]\n\n\n\n\n\ncat_items\n\n\nIPFSApi.cat_items\n\n IPFSApi.cat_items (cid:str, **kwargs)\n\nShow IPFS object data\n\n\n\n\nType\nDetails\n\n\n\n\ncid\nstr\nThe path to the IPFS object(s) to be output\n\n\nkwargs\n\n\n\n\n\nWhen given a file CID\n\napi.change_gateway_type = 'local'\n\nChanged to local node\n\n\n\nresponse, content = api.cat_items(cid='QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B'); content\n\n'# Welcome to Immerse by Algovera\\r\\n> A python library by Algovera to interact with IPFS and IPFS ecosystem such as the common pinning services\\r\\n\\r\\n\\r\\n## What is Immerse?\\r\\n\\r\\nImmerse is a python library by Algovera to interact with IPFS and IPFS ecosystem such as the common pinning services. It is designed by data scientists for data scientists to interact with the IPFS ecosystem without leaving the comfort of python and jupyter notebook.\\r\\n\\r\\nYou can learn more about IPFS [here](https://ipfs.io/#why)\\r\\n\\r\\nIPFS is built using the go-lang and javascript. With Immerse, you can interact with IPFS using the exposed [HTTP RPC API](https://docs.ipfs.io/reference/http/api/#getting-started). \\r\\n\\r\\nYou will need a local IPFS Node running to use the HTTP API (even when using Immerse). As an alternative, you can connect via the [Infura](https://infura.io/product/ipfs)\\'s dedicated IPFS gateway. Immerse provide both ways to interact with IPFS.\\r\\n\\r\\n## Installing\\r\\n\\r\\nto do: instructions on how to install library goes here\\r\\n\\r\\n## How to use\\r\\n\\r\\nTo adda file to IPFS, simply\\r\\n\\r\\n```python\\r\\nfrom immerse import httpapi\\r\\n\\r\\nurl = get_coreurl()\\r\\nresponse, json = add_items(url, \"path/to/file\")\\r\\n```\\r\\n'\n\n\nWhen given a directory CID (this will throw an error)\n\nresponse, content = api.cat_items(cid='QmeyTiqrD6oo4eQXbAt7hZn3ASzpcP3Kvb1rc9qznEowEw'); content\n\nHTTPError: Response Status Code: 500; Error Message: this dag node is a directory\n\n\n\napi.change_gateway_type = 'public'\n\nChanged to public node\n\n\n\nresponse, content = api.cat_items(cid='QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B'); content\n\n'# Welcome to Immerse by Algovera\\r\\n> A python library by Algovera to interact with IPFS and IPFS ecosystem such as the common pinning services\\r\\n\\r\\n\\r\\n## What is Immerse?\\r\\n\\r\\nImmerse is a python library by Algovera to interact with IPFS and IPFS ecosystem such as the common pinning services. It is designed by data scientists for data scientists to interact with the IPFS ecosystem without leaving the comfort of python and jupyter notebook.\\r\\n\\r\\nYou can learn more about IPFS [here](https://ipfs.io/#why)\\r\\n\\r\\nIPFS is built using the go-lang and javascript. With Immerse, you can interact with IPFS using the exposed [HTTP RPC API](https://docs.ipfs.io/reference/http/api/#getting-started). \\r\\n\\r\\nYou will need a local IPFS Node running to use the HTTP API (even when using Immerse). As an alternative, you can connect via the [Infura](https://infura.io/product/ipfs)\\'s dedicated IPFS gateway. Immerse provide both ways to interact with IPFS.\\r\\n\\r\\n## Installing\\r\\n\\r\\nto do: instructions on how to install library goes here\\r\\n\\r\\n## How to use\\r\\n\\r\\nTo adda file to IPFS, simply\\r\\n\\r\\n```python\\r\\nfrom immerse import httpapi\\r\\n\\r\\nurl = get_coreurl()\\r\\nresponse, json = add_items(url, \"path/to/file\")\\r\\n```\\r\\n'\n\n\n\napi.change_gateway_type = 'infura'\n\nChanged to infura node\n\n\n\nresponse, content = api.cat_items(cid='QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B'); content\n\n'# Welcome to Immerse by Algovera\\r\\n> A python library by Algovera to interact with IPFS and IPFS ecosystem such as the common pinning services\\r\\n\\r\\n\\r\\n## What is Immerse?\\r\\n\\r\\nImmerse is a python library by Algovera to interact with IPFS and IPFS ecosystem such as the common pinning services. It is designed by data scientists for data scientists to interact with the IPFS ecosystem without leaving the comfort of python and jupyter notebook.\\r\\n\\r\\nYou can learn more about IPFS [here](https://ipfs.io/#why)\\r\\n\\r\\nIPFS is built using the go-lang and javascript. With Immerse, you can interact with IPFS using the exposed [HTTP RPC API](https://docs.ipfs.io/reference/http/api/#getting-started). \\r\\n\\r\\nYou will need a local IPFS Node running to use the HTTP API (even when using Immerse). As an alternative, you can connect via the [Infura](https://infura.io/product/ipfs)\\'s dedicated IPFS gateway. Immerse provide both ways to interact with IPFS.\\r\\n\\r\\n## Installing\\r\\n\\r\\nto do: instructions on how to install library goes here\\r\\n\\r\\n## How to use\\r\\n\\r\\nTo adda file to IPFS, simply\\r\\n\\r\\n```python\\r\\nfrom immerse import httpapi\\r\\n\\r\\nurl = get_coreurl()\\r\\nresponse, json = add_items(url, \"path/to/file\")\\r\\n```\\r\\n'\n\n\n\n\n\nget_items\n\nresponse, content = api.get_items(cid='QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B'); content\n\n'QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x000000644\\x000000000\\x000000000\\x0000000002233\\x0014265706026\\x00016723\\x00 0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00ustar\\x0000\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x000000000\\x000000000\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00# Welcome to Immerse by Algovera\\r\\n> A python library by Algovera to interact with IPFS and IPFS ecosystem such as the common pinning services\\r\\n\\r\\n\\r\\n## What is Immerse?\\r\\n\\r\\nImmerse is a python library by Algovera to interact with IPFS and IPFS ecosystem such as the common pinning services. It is designed by data scientists for data scientists to interact with the IPFS ecosystem without leaving the comfort of python and jupyter notebook.\\r\\n\\r\\nYou can learn more about IPFS [here](https://ipfs.io/#why)\\r\\n\\r\\nIPFS is built using the go-lang and javascript. With Immerse, you can interact with IPFS using the exposed [HTTP RPC API](https://docs.ipfs.io/reference/http/api/#getting-started). \\r\\n\\r\\nYou will need a local IPFS Node running to use the HTTP API (even when using Immerse). As an alternative, you can connect via the [Infura](https://infura.io/product/ipfs)\\'s dedicated IPFS gateway. Immerse provide both ways to interact with IPFS.\\r\\n\\r\\n## Installing\\r\\n\\r\\nto do: instructions on how to install library goes here\\r\\n\\r\\n## How to use\\r\\n\\r\\nTo adda file to IPFS, simply\\r\\n\\r\\n```python\\r\\nfrom immerse import httpapi\\r\\n\\r\\nurl = get_coreurl()\\r\\nresponse, json = add_items(url, \"path/to/file\")\\r\\n```\\r\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n\n\n\n\nHow to download a directory from IPFS\n\nonly available on local and public\n\n\n\n\nDownloadDir\n\n DownloadDir (gateway_type:str, root_cid:str, output_fol:str)\n\nDownload a IPFS directory to your local disk\n\n\n\n\nType\nDetails\n\n\n\n\ngateway_type\nstr\nGateway to use - works on local and public\n\n\nroot_cid\nstr\nRoot CID of the directory\n\n\noutput_fol\nstr\nPath to save in your local disk\n\n\n\n\ndownload = DownloadDir('local', 'QmWJvSt7QGY2yhr9mrfAm76vRvxXA5ygB4zd2SmfD43vtX', 'output2')\n\nChanged to local node\n\n\n\ndownload.download()\n\n\ndownload.full_structure\n\n{'output2': {'output2/adult_data.csv': {'Hash': 'QmZnxARhJWsCbTxiAzoRhnxHgMtoEkNJNS8DGLCBEMvm4V',\n   'type': 2},\n  'output2/fol1': {'output2/fol1': {'output2/fol1/adult_data.csv': {'Hash': 'QmZnxARhJWsCbTxiAzoRhnxHgMtoEkNJNS8DGLCBEMvm4V',\n     'type': 2},\n    'output2/fol1/fol2': {'output2/fol1/fol2': {'output2/fol1/fol2/adult_data.csv': {'Hash': 'QmZnxARhJWsCbTxiAzoRhnxHgMtoEkNJNS8DGLCBEMvm4V',\n       'type': 2},\n      'output2/fol1/fol2/test.txt': {'Hash': 'QmTT8vwdbnP9Ls8bSY1LMyW4a8bEwTYZa5izEoJMBtTPfb',\n       'type': 2},\n      'output2/fol1/fol2/test2.txt': {'Hash': 'QmWa7aPQWCkLAmV2pt1Wjj3uE7V3CG3KYMKBKsVAvWG649',\n       'type': 2},\n      'output2/fol1/fol2/test3.txt': {'Hash': 'Qmb6W6nVPYd5CJKFpC1zGGuoD7TYQLE5PGG1RHkHm2W3m9',\n       'type': 2}}},\n    'output2/fol1/test.txt': {'Hash': 'QmTT8vwdbnP9Ls8bSY1LMyW4a8bEwTYZa5izEoJMBtTPfb',\n     'type': 2},\n    'output2/fol1/test2.txt': {'Hash': 'QmWa7aPQWCkLAmV2pt1Wjj3uE7V3CG3KYMKBKsVAvWG649',\n     'type': 2},\n    'output2/fol1/test3.txt': {'Hash': 'Qmb6W6nVPYd5CJKFpC1zGGuoD7TYQLE5PGG1RHkHm2W3m9',\n     'type': 2}}},\n  'output2/test.txt': {'Hash': 'QmTT8vwdbnP9Ls8bSY1LMyW4a8bEwTYZa5izEoJMBtTPfb',\n   'type': 2},\n  'output2/test2.txt': {'Hash': 'QmWa7aPQWCkLAmV2pt1Wjj3uE7V3CG3KYMKBKsVAvWG649',\n   'type': 2},\n  'output2/test3.txt': {'Hash': 'Qmb6W6nVPYd5CJKFpC1zGGuoD7TYQLE5PGG1RHkHm2W3m9',\n   'type': 2}}}"
  },
  {
    "objectID": "ipfshttpapi.html#dags",
    "href": "ipfshttpapi.html#dags",
    "title": "IPFS HTTP API",
    "section": "DAGs",
    "text": "DAGs\n\n\nIPFSApi.dag_get\n\n IPFSApi.dag_get (cid:str, output_codec:str='dag-json')\n\nGet a DAG node from IPFS.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncid\nstr\n\nThe path to the IPFS DAG node\n\n\noutput_codec\nstr\ndag-json\n\n\n\n\n\n\n\nIPFSApi.dag_export\n\n IPFSApi.dag_export (cid:str, **kwargs)\n\nStreams the selected DAG as a .car stream on stdout.\n\n\n\n\nType\nDetails\n\n\n\n\ncid\nstr\nThe path to the IPFS DAG node\n\n\nkwargs\n\n\n\n\n\n\n\n\nIPFSApi.dag_stat\n\n IPFSApi.dag_stat (cid:str, **kwargs)\n\nGets stats for a DAG.\n\n\n\n\nType\nDetails\n\n\n\n\ncid\nstr\nThe path to the IPFS DAG node\n\n\nkwargs\n\n\n\n\n\n\n\nMaking use of DAGs\nLet’s take a look at the DAG structure of the folder we uploaded earlier\n\nresponse, content = api.dag_get('QmeyTiqrD6oo4eQXbAt7hZn3ASzpcP3Kvb1rc9qznEowEw'); content\n\n{'Data': {'/': {'bytes': 'CAE'}},\n 'Links': [{'Hash': {'/': 'QmZnxARhJWsCbTxiAzoRhnxHgMtoEkNJNS8DGLCBEMvm4V'},\n   'Name': 'adult_data.csv',\n   'Tsize': 3975476},\n  {'Hash': {'/': 'QmUNLLsPACCz1vLxQVkXqqLX5R1X345qqfHbsf67hvA3Nn'},\n   'Name': 'fol1',\n   'Tsize': 4},\n  {'Hash': {'/': 'QmTT8vwdbnP9Ls8bSY1LMyW4a8bEwTYZa5izEoJMBtTPfb'},\n   'Name': 'test.txt',\n   'Tsize': 32},\n  {'Hash': {'/': 'QmWa7aPQWCkLAmV2pt1Wjj3uE7V3CG3KYMKBKsVAvWG649'},\n   'Name': 'test2.txt',\n   'Tsize': 31},\n  {'Hash': {'/': 'Qmb6W6nVPYd5CJKFpC1zGGuoD7TYQLE5PGG1RHkHm2W3m9'},\n   'Name': 'test3.txt',\n   'Tsize': 40}]}\n\n\ndag_get method returns a dict with data and links. data is not exciting for a directory. Under the links, we can see files/directories that are in the uploaded directory.\nLet’s see the number of files/directories in the links\n\nlen(content['Links'])\n\n5\n\n\nLet’s take a look at the first link\n\ncontent['Links'][0]\n\n{'Hash': {'/': 'QmZnxARhJWsCbTxiAzoRhnxHgMtoEkNJNS8DGLCBEMvm4V'},\n 'Name': 'adult_data.csv',\n 'Tsize': 3975476}"
  },
  {
    "objectID": "ipfshttpapi.html#local-pinning",
    "href": "ipfshttpapi.html#local-pinning",
    "title": "IPFS HTTP API",
    "section": "Local Pinning",
    "text": "Local Pinning\n\nPin an IPFS object locally\nMore info on this can be found here\n\n\nIPFSApi.pin_add\n\n IPFSApi.pin_add (cid:str, recursive:str='true')\n\nPin objects to local storage.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncid\nstr\n\nPath to IPFS object(s) to be pinned\n\n\nrecursive\nstr\ntrue\nRecursively pin the object linked to by the specified object(s)\n\n\n\n\n\n\nIPFSApi.pin_ls\n\n IPFSApi.pin_ls (type_:str='all', **kwargs)\n\nList objects pinned to local storage.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntype_\nstr\nall\nThe type of pinned keys to list. Can be “direct”, “indirect”, “recursive”, or “all”\n\n\nkwargs\n\n\n\n\n\n\n\n\n\nIPFSApi.pin_rm\n\n IPFSApi.pin_rm (cid:str, recursive:str='true', **kwargs)\n\nList objects pinned to local storage.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncid\nstr\n\nPath to object(s) to be unpinned\n\n\nrecursive\nstr\ntrue\nRecursively unpin the object linked to by the specified object(s)\n\n\nkwargs\n\n\n\n\n\n\n\nresponse, content = api.pin_add('QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B'); content\n\n[{'Pins': ['QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B']}]\n\n\nLet’s list all the items pinned locally.\n\nresponse, content = api.pin_ls(); len(content[0]['Keys'])\n\n184\n\n\n\nlist(content[0]['Keys'].keys())[:10]\n\n['QmNQw55u9tgykLVkb5W7nSWQuEK3ZbX8gx4fBpWDot3D27',\n 'QmNRupDY9nxDmCYecXMc656bbuM3ajZGXW4tL8nzgXccjM',\n 'QmNccC85TwgHqCu7rrbQuAiUqxoiFFQh2rFXz2d3cuXwG1',\n 'QmNyDNFfiF8y8xGNuNh6bL63FzAj1m79Mo8v8Y9aR3WCyx',\n 'QmNya3hiMsz5M3VBZ1UPDB1vDw57w3BKZhQRdAvA7znBx9',\n 'QmNyb437zLGoaj4rfbn8yUccTH6c6hR7XeeYcVitwc2iuJ',\n 'QmP2ZvQKrjvmNBWRMboFYzTWKSA2RR2yecoEtkS9HkdqpX',\n 'QmP6611DeM6Y1DTbkm6U7KwndWq1hnzA7wuAMwLT8VPnnX',\n 'QmPCxQWmfe14hTp86YAxbiG49HiSFxRLd4htPzpK7ivnge',\n 'QmPJDYxahQaiSMuAWjiYusmBte2sy66qk584ukn3NnKiW7']\n\n\nThe item we added above is also part of it.\n\ncontent[0]['Keys']['QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B']\n\n{'Type': 'recursive'}\n\n\nWhat does the Type : recursive means? - Direct pins - Single block and no others in relation to it. - Recursive pins - Given block and all of its children. - Indirect pins - the result of a given block’s parent being pinned recursively.\n\nresponse, content = api.pin_rm('QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B'); response.status_code\n\n200"
  },
  {
    "objectID": "ipfshttpapi.html#remote-pinning-service",
    "href": "ipfshttpapi.html#remote-pinning-service",
    "title": "IPFS HTTP API",
    "section": "Remote Pinning Service",
    "text": "Remote Pinning Service\n\n\nIPFSApi.rspin_add\n\n IPFSApi.rspin_add (service_name:str, service_edpt:str, service_key:str)\n\nPin object to remote pinning service.\n\n\n\n\nType\nDetails\n\n\n\n\nservice_name\nstr\nName of the remote pinning service to use\n\n\nservice_edpt\nstr\nService endpoint\n\n\nservice_key\nstr\nService key\n\n\n\n\n\n\nIPFSApi.rspin_ls\n\n IPFSApi.rspin_ls (**kwargs)\n\nList remote pinning services.\n\n\n\nIPFSApi.rspin_rm\n\n IPFSApi.rspin_rm (service_name:str)\n\nRemove remote pinning service.\n\n\n\n\nType\nDetails\n\n\n\n\nservice_name\nstr\nName of pinning service to remove\n\n\n\n\n\nWork with remote pinning service\nMore info on this can be found here and here\nAdding your Pinata to your IPFS node 1. Create an account with Pinata 2. use rspin_add method to add the service\n\nimport os\nos.environ[\"pinata_api_secret\"] = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VySW5mb3JtYXRpb24iOnsiaWQiOiJhZDkxMTQ3Yy0yMmE5LTQ1MjgtODk2OS05ZTdjMjAwNzE2ZjAiLCJlbWFpbCI6Im1hcnNoYXRoQGdtYWlsLmNvbSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJwaW5fcG9saWN5Ijp7InJlZ2lvbnMiOlt7ImlkIjoiRlJBMSIsImRlc2lyZWRSZXBsaWNhdGlvbkNvdW50IjoxfV0sInZlcnNpb24iOjF9LCJtZmFfZW5hYmxlZCI6ZmFsc2V9LCJhdXRoZW50aWNhdGlvblR5cGUiOiJzY29wZWRLZXkiLCJzY29wZWRLZXlLZXkiOiIxMjc4NDQ5OGY3YzI0ZjczNDBjMiIsInNjb3BlZEtleVNlY3JldCI6ImQyYmNlZjFlODVlZjgyOTcwNGE0ZTk4NGRmYjc3ZWE0MDhlY2NkODk1MjMxMzI2YjVlMmZlZDQxMTBhZmQyMGEiLCJpYXQiOjE2NTM5Njk0MDN9.Fa055BjXNmERyl_ZAA9NJscdt0HqbiX0ByY8pU5uLNY'\n\n\nNote: For pinata the service_key is also called JWT\n\n\n\n\nimage.png\n\n\n\nres, content = api.rspin_add('pinata', 'https://api.pinata.cloud/psa', os.environ['pinata_api_secret']);res.status_code\n\n200\n\n\nLet’s list the pinned services to see if we successfully added pinata\nPinata has been added\n\nresponse, content = api.rspin_ls(); content\n\n[{'RemoteServices': [{'Service': 'pinata',\n    'ApiEndpoint': 'https://api.pinata.cloud/psa'}]}]"
  },
  {
    "objectID": "ipfshttpapi.html#remote-pinning",
    "href": "ipfshttpapi.html#remote-pinning",
    "title": "IPFS HTTP API",
    "section": "Remote Pinning",
    "text": "Remote Pinning\n\n\nIPFSApi.rpin_add\n\n IPFSApi.rpin_add (cid:str, service:str, background:str='false', **kwargs)\n\nPin object to remote pinning service.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncid\nstr\n\nPath to IPFS object(s) to be pinned\n\n\nservice\nstr\n\nName of the remote pinning service to use\n\n\nbackground\nstr\nfalse\nAdd to the queue on the remote service and return immediately (does not wait for pinned status)\n\n\nkwargs\n\n\n\n\n\n\n\n\n\nIPFSApi.rpin_ls\n\n IPFSApi.rpin_ls (service:str, **kwargs)\n\nList objects pinned to remote pinning service.\n\n\n\n\nType\nDetails\n\n\n\n\nservice\nstr\nName of the remote pinning service to use\n\n\nkwargs\n\n\n\n\n\n\n\n\nIPFSApi.rpin_rm\n\n IPFSApi.rpin_rm (service:str, **kwargs)\n\nRemove pins from remote pinning service.\n\n\n\n\nType\nDetails\n\n\n\n\nservice\nstr\nName of the remote pinning service to use\n\n\nkwargs\n\n\n\n\n\nLet’s add a new file to IPFS and pin it to pinata.\n\nresponse, content = api.add_items(filepath='output/test.txt'); content\n\n[{'Name': 'test.txt', 'Bytes': 24},\n {'Name': 'test.txt',\n  'Hash': 'QmTT8vwdbnP9Ls8bSY1LMyW4a8bEwTYZa5izEoJMBtTPfb',\n  'Size': '32'}]\n\n\n\nresponse, content = api.rpin_add('QmTT8vwdbnP9Ls8bSY1LMyW4a8bEwTYZa5izEoJMBtTPfb', 'pinata'); response.status_code\n\nHTTPError: Response Status Code: 500; Error Message: reason: \"DUPLICATE_OBJECT\", details: \"Object already pinned to pinata. Please remove or replace existing pin object.\": 400 Bad Request\n\n\nAs can ber seen, we have successfully added the file to pinata\n\nresponse, content = api.rpin_ls('pinata'); content\n\n[{'Status': 'pinned',\n  'Cid': 'QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B',\n  'Name': ''},\n {'Status': 'pinned',\n  'Cid': 'QmUNLLsPACCz1vLxQVkXqqLX5R1X345qqfHbsf67hvA3Nn',\n  'Name': 'policy/12D3KooWPuYpxNLqniaG7pxknEfRLxxYFUXULKXi3bE5zbCFDdCZ/mfs'},\n {'Status': 'pinned',\n  'Cid': 'Qmb6W6nVPYd5CJKFpC1zGGuoD7TYQLE5PGG1RHkHm2W3m9',\n  'Name': ''},\n {'Status': 'pinned',\n  'Cid': 'QmeGotBL5dsmgryJLn2q3krUCjX6F8sBMBBEC6ayknHVYZ',\n  'Name': ''},\n {'Status': 'pinned',\n  'Cid': 'QmTT8vwdbnP9Ls8bSY1LMyW4a8bEwTYZa5izEoJMBtTPfb',\n  'Name': ''}]"
  },
  {
    "objectID": "ipfshttpapi.html#block",
    "href": "ipfshttpapi.html#block",
    "title": "IPFS HTTP API",
    "section": "Block",
    "text": "Block\n\n\nIPFSApi.block_get\n\n IPFSApi.block_get (cid:str)\n\nGet a raw IPFS block.\n\n\n\n\nType\nDetails\n\n\n\n\ncid\nstr\nThe base58 multihash of an existing block to get\n\n\n\n\n\n\nIPFSApi.block_put\n\n IPFSApi.block_put (filepath:str, mhtype:str='sha2-256', mhlen:int=-1,\n                    pin:str=False, **kwargs)\n\nStore input as an IPFS block.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfilepath\nstr\n\nPath to file\n\n\nmhtype\nstr\nsha2-256\nmultihash hash function.\n\n\nmhlen\nint\n-1\nMultihash hash length\n\n\npin\nstr\nFalse\npin added blocks recursively\n\n\nkwargs\n\n\n\n\n\n\n\n\n\nIPFSApi.block_rm\n\n IPFSApi.block_rm (cid:str, force:str='false', quiet:str='false')\n\nRemove IPFS block(s).\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncid\nstr\n\nBash58 encoded multihash of block(s) to remove\n\n\nforce\nstr\nfalse\nIgnore nonexistent blocks.\n\n\nquiet\nstr\nfalse\nWrite minimal output.\n\n\n\n\n\n\nIPFSApi.block_stat\n\n IPFSApi.block_stat (cid:str)\n\nPrint information of a raw IPFS block.\n\n\n\n\nType\nDetails\n\n\n\n\ncid\nstr\nBash58 encoded multihash of block(s) to remove\n\n\n\n\n\nWork with blocks\nTo explore the blocks modeule, let’s add a large file\n\nresponse, content = api.add_items('output/adult_data.csv'); content\n\n[{'Name': 'adult_data.csv', 'Bytes': 262144},\n {'Name': 'adult_data.csv', 'Bytes': 524288},\n {'Name': 'adult_data.csv', 'Bytes': 786432},\n {'Name': 'adult_data.csv', 'Bytes': 1048576},\n {'Name': 'adult_data.csv', 'Bytes': 1310720},\n {'Name': 'adult_data.csv', 'Bytes': 1572864},\n {'Name': 'adult_data.csv', 'Bytes': 1835008},\n {'Name': 'adult_data.csv', 'Bytes': 2097152},\n {'Name': 'adult_data.csv', 'Bytes': 2359296},\n {'Name': 'adult_data.csv', 'Bytes': 2621440},\n {'Name': 'adult_data.csv', 'Bytes': 2883584},\n {'Name': 'adult_data.csv', 'Bytes': 3145728},\n {'Name': 'adult_data.csv', 'Bytes': 3407872},\n {'Name': 'adult_data.csv', 'Bytes': 3670016},\n {'Name': 'adult_data.csv', 'Bytes': 3932160},\n {'Name': 'adult_data.csv', 'Bytes': 3974475},\n {'Name': 'adult_data.csv',\n  'Hash': 'QmZnxARhJWsCbTxiAzoRhnxHgMtoEkNJNS8DGLCBEMvm4V',\n  'Size': '3975476'}]\n\n\nAs you can see from below, adding it results in blocks and we are given the root CID\nWe can take a look at the CIDs of different block like below\n\nresponse, content = api.ls_items('QmZnxARhJWsCbTxiAzoRhnxHgMtoEkNJNS8DGLCBEMvm4V');content\n\n[{'Objects': [{'Hash': 'QmZnxARhJWsCbTxiAzoRhnxHgMtoEkNJNS8DGLCBEMvm4V',\n    'Links': [{'Name': '',\n      'Hash': 'Qmc7fYo65HLt1bwN6GudwZfq5MrAGYQ2wwsQ9y1fh6zNz9',\n      'Size': 262144,\n      'Type': 2,\n      'Target': ''},\n     {'Name': '',\n      'Hash': 'QmY1DtAnTrDX3KPgHFMPk6Zapa53s1fCapXChLBN9sMSQ4',\n      'Size': 262144,\n      'Type': 2,\n      'Target': ''},\n     {'Name': '',\n      'Hash': 'QmY6TEEHe4WiJVAcY6Lo9shWfc8WVQHV6HtBE8HaEfYykV',\n      'Size': 262144,\n      'Type': 2,\n      'Target': ''},\n     {'Name': '',\n      'Hash': 'QmRw8SE71wXbRYr4pttxJAhHCcxsTLqdimkNEPUkmJoppe',\n      'Size': 262144,\n      'Type': 2,\n      'Target': ''},\n     {'Name': '',\n      'Hash': 'QmYCUnARWF3jesUjc23Ku2sdxstoNjkzgevPcG95skkzq3',\n      'Size': 262144,\n      'Type': 2,\n      'Target': ''},\n     {'Name': '',\n      'Hash': 'Qmdc41AcSjYZ9qazoFdeWY9zrEEQv5e3mp2wuTbbMBVgis',\n      'Size': 262144,\n      'Type': 2,\n      'Target': ''},\n     {'Name': '',\n      'Hash': 'QmUzwuZkN7HMHXa169vZAZDcMu9DGzPUDTCbgcJomboYDs',\n      'Size': 262144,\n      'Type': 2,\n      'Target': ''},\n     {'Name': '',\n      'Hash': 'QmYNW7vP16AA9DvoH1t2598KehqmefQTAnn2XBRDy4xbRZ',\n      'Size': 262144,\n      'Type': 2,\n      'Target': ''},\n     {'Name': '',\n      'Hash': 'QmWzwegNQkJg1UJjeyAJpeVK58yxAzexfgPVgwG2GjaXrc',\n      'Size': 262144,\n      'Type': 2,\n      'Target': ''},\n     {'Name': '',\n      'Hash': 'QmcKhLKvYXCK1aFzkNiF5tZ8YPNHuiESdqujcKm6N4A7TZ',\n      'Size': 262144,\n      'Type': 2,\n      'Target': ''},\n     {'Name': '',\n      'Hash': 'QmYFUbBW93ZXhRw9gLmwodEeG26oHeo13RiHLDEmSitepf',\n      'Size': 262144,\n      'Type': 2,\n      'Target': ''},\n     {'Name': '',\n      'Hash': 'QmNyb437zLGoaj4rfbn8yUccTH6c6hR7XeeYcVitwc2iuJ',\n      'Size': 262144,\n      'Type': 2,\n      'Target': ''},\n     {'Name': '',\n      'Hash': 'QmeEPW7tFvgbJhrUEQrgHRkGBWx3p6HhsCJLEZ3LEduW64',\n      'Size': 262144,\n      'Type': 2,\n      'Target': ''},\n     {'Name': '',\n      'Hash': 'QmZ6Pe8xKLQJFzE28w6G4Lh6afDW3W4Hc8oHEFmVm88ouY',\n      'Size': 262144,\n      'Type': 2,\n      'Target': ''},\n     {'Name': '',\n      'Hash': 'Qmf4n9eT4r8eFeT92RjKXwWvp8TcxChiuweG74x3wZmVDy',\n      'Size': 262144,\n      'Type': 2,\n      'Target': ''},\n     {'Name': '',\n      'Hash': 'QmPSkoihW58K3v6n4UbKwu6Ex861Mf9xDakXhAD8uLLhXs',\n      'Size': 42315,\n      'Type': 2,\n      'Target': ''}]}]}]\n\n\n\nblocks = content[0][\"Objects\"][0][\"Links\"]\n\n\nprint(f'Number of blocks: {len(blocks)}')\n\nNumber of blocks: 16\n\n\nHere, we can see the three blocks of the total 16 blocks.\n\nblocks[:3]\n\n[{'Name': '',\n  'Hash': 'Qmc7fYo65HLt1bwN6GudwZfq5MrAGYQ2wwsQ9y1fh6zNz9',\n  'Size': 262144,\n  'Type': 2,\n  'Target': ''},\n {'Name': '',\n  'Hash': 'QmY1DtAnTrDX3KPgHFMPk6Zapa53s1fCapXChLBN9sMSQ4',\n  'Size': 262144,\n  'Type': 2,\n  'Target': ''},\n {'Name': '',\n  'Hash': 'QmY6TEEHe4WiJVAcY6Lo9shWfc8WVQHV6HtBE8HaEfYykV',\n  'Size': 262144,\n  'Type': 2,\n  'Target': ''}]\n\n\nLet’s read whats in the first and second block\n\nresponse, content = api.cat_items(blocks[0]['Hash'])\n\n\nprint(response.text[:100]); len(response.text)\n\nage, workclass, fnlwgt, education, education-num, marital-status, occupation, relationship, race, se\n\n\n262144\n\n\n\nresponse, content = api.cat_items(blocks[1]['Hash'])\n\n\nprint(response.text[:100]); len(response.text)\n\nnited-States, <=50K\n28, Private, 201175, 11th, 7, Never-married, Machine-op-inspct, Not-in-family, B\n\n\n262144\n\n\nWe can also read using the cat method under the blocks module in which case if block is an intermediate block it will only print the raw binary data.\n\nresponse, content = api.block_get(blocks[0]['Hash'])\n\n\nresponse.text[:300]\n\n'\\n\\x8a\\x80\\x10\\x08\\x02\\x12\\x80\\x80\\x10age, workclass, fnlwgt, education, education-num, marital-status, occupation, relationship, race, sex, capital-gain, capital-loss, hours-per-week, native-country, salary\\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States'\n\n\n\nAdd a block\n\nresponse, content = api.block_put('../README.md'); content\n\n[{'Key': 'bafkreicb2n4nhac6nwsviqe2ik4pt22ukewf7zfz2j2aof2au3a2qgi3oa',\n  'Size': 1179}]\n\n\n\n\nGet stat on a block\n\nresponse, content = api.block_stat('bafkreicb2n4nhac6nwsviqe2ik4pt22ukewf7zfz2j2aof2au3a2qgi3oa'); content\n\n[{'Key': 'bafkreicb2n4nhac6nwsviqe2ik4pt22ukewf7zfz2j2aof2au3a2qgi3oa',\n  'Size': 1179}]\n\n\n\n\nRemove a block\n\nresponse, content = api.block_rm('bafkreicb2n4nhac6nwsviqe2ik4pt22ukewf7zfz2j2aof2au3a2qgi3oa'); response.status_code\n\n200"
  },
  {
    "objectID": "ipfshttpapi.html#mutable-file-system-files",
    "href": "ipfshttpapi.html#mutable-file-system-files",
    "title": "IPFS HTTP API",
    "section": "Mutable File System (files)",
    "text": "Mutable File System (files)\n\n\nIPFSApi.mfs_chcid\n\n IPFSApi.mfs_chcid (path:str='/', cid_version:int=0, **kwargs)\n\nChange the CID version or hash function of the root node of a given path.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n/\nPath to change\n\n\ncid_version\nint\n0\nCid version to use\n\n\nkwargs\n\n\n\n\n\n\n\n\n\nIPFSApi.mfs_cp\n\n IPFSApi.mfs_cp (source_path:str, dest_path:str, **kwargs)\n\nAdd references to IPFS files and directories in MFS (or copy within MFS).\n\n\n\n\nType\nDetails\n\n\n\n\nsource_path\nstr\nSource IPFS or MFS path to copy\n\n\ndest_path\nstr\nDestination within MFS\n\n\nkwargs\n\n\n\n\n\n\n\n\nIPFSApi.mfs_flush\n\n IPFSApi.mfs_flush (path:str='/')\n\nFlush a given path’s data to disk\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n/\nPath to flush\n\n\n\n\n\n\nIPFSApi.mfs_ls\n\n IPFSApi.mfs_ls (path:str='/', **kwargs)\n\nList directories in the local mutable namespace.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n/\nPath to show listing for\n\n\nkwargs\n\n\n\n\n\n\n\n\n\nIPFSApi.mfs_mkdir\n\n IPFSApi.mfs_mkdir (path:str, **kwargs)\n\nMake directories.\n\n\n\n\nType\nDetails\n\n\n\n\npath\nstr\nPath to dir to make\n\n\nkwargs\n\n\n\n\n\n\n\n\nIPFSApi.mfs_mv\n\n IPFSApi.mfs_mv (source_path:str, dest_path:str)\n\nMove files.\n\n\n\n\nType\nDetails\n\n\n\n\nsource_path\nstr\nSource file to move\n\n\ndest_path\nstr\nDestination path for file to be moved to\n\n\n\n\n\n\nIPFSApi.mfs_read\n\n IPFSApi.mfs_read (path, **kwargs)\n\nRead a file in a given MFS.\n\n\n\n\nDetails\n\n\n\n\npath\nPath to file to be read\n\n\nkwargs\n\n\n\n\n\n\n\nIPFSApi.mfs_rm\n\n IPFSApi.mfs_rm (path, **kwargs)\n\nRead a file in a given MFS.\n\n\n\n\nDetails\n\n\n\n\npath\nFile to remove\n\n\nkwargs\n\n\n\n\n\n\n\nIPFSApi.mfs_stat\n\n IPFSApi.mfs_stat (path, **kwargs)\n\nDisplay file status.\n\n\n\n\nDetails\n\n\n\n\npath\nPath to node to stat\n\n\nkwargs\n\n\n\n\n\n\n\nIPFSApi.mfs_write\n\n IPFSApi.mfs_write (path, filepath, create=True, **kwargs)\n\nDisplay file status.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\n\n\nPath to write to\n\n\nfilepath\n\n\nFile to add\n\n\ncreate\nbool\nTrue\nCreate the file if it does not exist\n\n\nkwargs\n\n\n\n\n\n\n\n\nWorking with MFS\nMaking a directory\n\nresponse, content = api.mfs_mkdir('/test'); response.status_code\n\n200"
  },
  {
    "objectID": "04_tutorial.fastai.html",
    "href": "04_tutorial.fastai.html",
    "title": "Tutorial - IPFS + ML",
    "section": "",
    "text": "In this tutorial, we will see how we can use IPFS as the storage for datasets and model in ML workflow."
  },
  {
    "objectID": "04_tutorial.fastai.html#add-a-dataset-to-ipfs-using-infura-node",
    "href": "04_tutorial.fastai.html#add-a-dataset-to-ipfs-using-infura-node",
    "title": "Tutorial - IPFS + ML",
    "section": "Add a dataset to IPFS using infura node",
    "text": "Add a dataset to IPFS using infura node\n\napi = IPFSApi()\n\nChanged to local node\n\n\n\napi.change_gateway_type = 'infura'\n\nChanged to infura node\n\n\n\nres, obj = api.add_items('output/adult_data.csv'); obj\n\n[{'Name': 'adult_data.csv', 'Bytes': 262144},\n {'Name': 'adult_data.csv', 'Bytes': 524288},\n {'Name': 'adult_data.csv', 'Bytes': 786432},\n {'Name': 'adult_data.csv', 'Bytes': 1048576},\n {'Name': 'adult_data.csv', 'Bytes': 1310720},\n {'Name': 'adult_data.csv', 'Bytes': 1572864},\n {'Name': 'adult_data.csv', 'Bytes': 1835008},\n {'Name': 'adult_data.csv', 'Bytes': 2097152},\n {'Name': 'adult_data.csv', 'Bytes': 2359296},\n {'Name': 'adult_data.csv', 'Bytes': 2621440},\n {'Name': 'adult_data.csv', 'Bytes': 2883584},\n {'Name': 'adult_data.csv', 'Bytes': 3145728},\n {'Name': 'adult_data.csv', 'Bytes': 3407872},\n {'Name': 'adult_data.csv', 'Bytes': 3670016},\n {'Name': 'adult_data.csv', 'Bytes': 3932160},\n {'Name': 'adult_data.csv', 'Bytes': 3974475},\n {'Name': 'adult_data.csv',\n  'Hash': 'QmZnxARhJWsCbTxiAzoRhnxHgMtoEkNJNS8DGLCBEMvm4V',\n  'Size': '3975476'}]"
  },
  {
    "objectID": "04_tutorial.fastai.html#retrieving-a-dataset-from-ipfs-using-public-node",
    "href": "04_tutorial.fastai.html#retrieving-a-dataset-from-ipfs-using-public-node",
    "title": "Tutorial - IPFS + ML",
    "section": "Retrieving a dataset from IPFS using public node",
    "text": "Retrieving a dataset from IPFS using public node\n\napi.change_gateway_type = 'public'\n\nChanged to public node\n\n\n\nres, obj = api.cat_items('QmZnxARhJWsCbTxiAzoRhnxHgMtoEkNJNS8DGLCBEMvm4V')\n\n\nwith open('output/adult_data_dl.csv', 'wb') as f:\n    f.write(res.content)\n\n\ndf = pd.read_csv('output/adult_data_dl.csv')\n\n\ndf.columns = [col.strip() for col in df.columns]"
  },
  {
    "objectID": "04_tutorial.fastai.html#building-a-tabular-model",
    "href": "04_tutorial.fastai.html#building-a-tabular-model",
    "title": "Tutorial - IPFS + ML",
    "section": "Building a tabular model",
    "text": "Building a tabular model\n\nsplits = RandomSplitter()(range_of(df))\ncat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\ncont_names = ['age', 'fnlwgt', 'education-num']\nprocs = [Categorify, FillMissing, Normalize]\ny_names = 'salary'\ny_block = CategoryBlock()\n\n\nto = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=cont_names,\n                   y_names=y_names, y_block=y_block, splits=splits)\n\ndls = to.dataloaders(bs=64)\n\n\nlearn = tabular_learner(dls, [200,100], metrics=accuracy)\n\n\nlearn.fit_one_cycle(3, 1e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.376169\n      0.351168\n      0.838913\n      00:02\n    \n    \n      1\n      0.368451\n      0.349306\n      0.836456\n      00:02\n    \n    \n      2\n      0.357292\n      0.343430\n      0.840756\n      00:02\n    \n  \n\n\n\n\nlearn.export('output/testmodel.pkl')"
  },
  {
    "objectID": "04_tutorial.fastai.html#adding-modelconfig_files-to-ipfs",
    "href": "04_tutorial.fastai.html#adding-modelconfig_files-to-ipfs",
    "title": "Tutorial - IPFS + ML",
    "section": "Adding model+config_files to IPFS",
    "text": "Adding model+config_files to IPFS\n\napi.change_gateway_type = 'infura'\n\nChanged to infura node\n\n\n\nres, obj = api.add_items('output/testmodel.pkl'); obj\n\n[{'Name': 'testmodel.pkl', 'Bytes': 242379},\n {'Name': 'testmodel.pkl',\n  'Hash': 'QmR77qXp7CYEg6kHA3z77mcayTmm9hoXz7YQHFz9WjphiE',\n  'Size': '242393'}]"
  },
  {
    "objectID": "04_tutorial.fastai.html#retrieving-modelconfig_files-from-ipfs",
    "href": "04_tutorial.fastai.html#retrieving-modelconfig_files-from-ipfs",
    "title": "Tutorial - IPFS + ML",
    "section": "Retrieving model+config_files from IPFS",
    "text": "Retrieving model+config_files from IPFS\n\napi.change_gateway_type = 'public'\n\nChanged to public node\n\n\n\nres, obj = api.cat_items('QmR77qXp7CYEg6kHA3z77mcayTmm9hoXz7YQHFz9WjphiE')\n\n\nwith open('output/testmodel_dl.pkl', 'wb') as f:\n    f.write(res.content)\n\n\nlearn = load_learner('output/testmodel_dl.pkl')\n\n\ndl = learn.dls.test_dl(df.iloc[:10])\n\n\nlearn.get_preds(dl=dl)\n\n\n\n\n\n\n\n\n(tensor([[0.9282, 0.0718],\n         [0.3556, 0.6444],\n         [0.9667, 0.0333],\n         [0.8731, 0.1269],\n         [0.4372, 0.5628],\n         [0.1317, 0.8683],\n         [0.9819, 0.0181],\n         [0.5074, 0.4926],\n         [0.8733, 0.1267],\n         [0.1803, 0.8197]]),\n tensor([[0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [1],\n         [1]], dtype=torch.int8))"
  },
  {
    "objectID": "pinataapi.html",
    "href": "pinataapi.html",
    "title": "Pinata",
    "section": "",
    "text": "generate_apikey\n\n generate_apikey (cred:str, key_name:str, pinlist:bool=False,\n                  userPinnedDataTotal:bool=False, hashMetadata:bool=True,\n                  hashPinPolicy:bool=False, pinByHash:bool=True,\n                  pinFileToIPFS:bool=True, pinJSONToIPFS:bool=True,\n                  pinJobs:bool=True, unpin:bool=True,\n                  userPinPolicy:bool=True)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncred\nstr\n\nJWT\n\n\nkey_name\nstr\n\nKey name\n\n\npinlist\nbool\nFalse\nlist pins\n\n\nuserPinnedDataTotal\nbool\nFalse\ntotal data stored\n\n\nhashMetadata\nbool\nTrue\nmetadata\n\n\nhashPinPolicy\nbool\nFalse\npolicy\n\n\npinByHash\nbool\nTrue\npin cid\n\n\npinFileToIPFS\nbool\nTrue\nupload file to IPFS\n\n\npinJSONToIPFS\nbool\nTrue\nupload json to IPFS\n\n\npinJobs\nbool\nTrue\nsee pin jobs\n\n\nunpin\nbool\nTrue\nunpin ipfs cid\n\n\nuserPinPolicy\nbool\nTrue\nestablish pin policy\n\n\n\nGenerate API Key\n\nresponse = generate_apikey(creds,\"Test\")\nresponse.json()\n\n{'pinata_api_key': '443e2dadf1e6a47c2754',\n 'pinata_api_secret': '0f84d67fe64136d4d9ddbfb1235a8ab1fd490f12c1f6fcc532b758062695a1dc',\n 'JWT': 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VySW5mb3JtYXRpb24iOnsiaWQiOiJkMjhmMjBjZi1kZmUwLTRjOGYtOTI3Mi0yNmI5YzJkOGEzY2QiLCJlbWFpbCI6InZpbnRhZ2Vnb2xkMTIzQGdtYWlsLmNvbSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJwaW5fcG9saWN5Ijp7InJlZ2lvbnMiOlt7ImlkIjoiTllDMSIsImRlc2lyZWRSZXBsaWNhdGlvbkNvdW50IjoxfV0sInZlcnNpb24iOjF9LCJtZmFfZW5hYmxlZCI6ZmFsc2UsInN0YXR1cyI6IkFDVElWRSJ9LCJhdXRoZW50aWNhdGlvblR5cGUiOiJzY29wZWRLZXkiLCJzY29wZWRLZXlLZXkiOiI0NDNlMmRhZGYxZTZhNDdjMjc1NCIsInNjb3BlZEtleVNlY3JldCI6IjBmODRkNjdmZTY0MTM2ZDRkOWRkYmZiMTIzNWE4YWIxZmQ0OTBmMTJjMWY2ZmNjNTMyYjc1ODA2MjY5NWExZGMiLCJpYXQiOjE2NTg0OTk3NDF9.Rd4F1zBE8wTPIwi4mD1x3pdEnfEM2Ik7zjiw0vQ7AC8'}\n\n\n\n\n\nlist_apikeys\n\n list_apikeys (cred:str)\n\nList API Keys\n\nnewest_apikey = list_apikeys(creds).json()[\"keys\"][0];newest_apikey\n\n{'id': '1ff9d58d-d7fa-4598-9863-c53a806faff6',\n 'name': 'Test',\n 'key': '443e2dadf1e6a47c2754',\n 'secret': 'ed7a3bdc89fb353b6c4f4155a9adb7f5:d15adb31f816663e2612ff5f89a5497abc30a5100ab83575247ba67cc9d49a7d61293d77c94049a8d83a82d8f02423357f311b26204bb57e7be9181d324e6424',\n 'max_uses': None,\n 'uses': 0,\n 'user_id': 'd28f20cf-dfe0-4c8f-9272-26b9c2d8a3cd',\n 'scopes': {'endpoints': {'data': {'pinList': False,\n    'userPinnedDataTotal': False},\n   'pinning': {'unpin': True,\n    'pinJobs': True,\n    'pinByHash': True,\n    'hashMetadata': True,\n    'hashPinPolicy': False,\n    'pinFileToIPFS': True,\n    'pinJSONToIPFS': True,\n    'userPinPolicy': True}}},\n 'revoked': False,\n 'createdAt': '2022-07-22T14:22:21.191Z',\n 'updatedAt': '2022-07-22T14:22:21.191Z'}\n\n\n\n\n\nrevoke_apikey\n\n revoke_apikey (cred:str, revoke_apikey:str)\n\nRevoke API Key\n\nrevoke_apikey(creds,response.json()[\"JWT\"])\n\n<Response [200]>\n\n\n\n\n\nupload_file\n\n upload_file (cred:str, name:str, fpaths:list, metadata:dict,\n              cid_version:str='1', directory:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncred\nstr\n\nJWT key\n\n\nname\nstr\n\nfilename\n\n\nfpaths\nlist\n\nfilepaths\n\n\nmetadata\ndict\n\nmetadata\n\n\ncid_version\nstr\n1\nIPFS cid\n\n\ndirectory\nbool\nFalse\nupload directory\n\n\n\nUpload File to IPFS with metadata\n\nmetadata = {\"type\":\"AdultData\"}\n\nupload_file(creds,\"adult_data.csv\",\"output/adult_data.csv\",metadata,cid_version=1,directory=False).text\n\n'{\"IpfsHash\":\"bafybeibcsnxt2sqzsvfg4lzg3iuxkfit37l3rz5gqmdxwsgec56qalmxie\",\"PinSize\":3975284,\"Timestamp\":\"2022-06-17T22:43:08.303Z\",\"isDuplicate\":true}'\n\n\n\n\n\nupload_jsonfile\n\n upload_jsonfile (cred:str, name:str, fpaths:list, metadata:dict,\n                  cid_version:str, directory:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncred\nstr\n\nJWT key\n\n\nname\nstr\n\nfilename\n\n\nfpaths\nlist\n\nfilepaths\n\n\nmetadata\ndict\n\nmetadata\n\n\ncid_version\nstr\n\nIPFS cid\n\n\ndirectory\nbool\nFalse\nupload directory\n\n\n\nThis endpoint is optimized for JSON files\n\nmetadata = {\"name\":\"Vote\"}\n\nupload_jsonfile(creds,\"ens_airdrop_Nov8th2021.json\",\"output/ens_airdrop_Nov8th2021.json\",metadata,cid_version=1,directory=False).text\n\n'{\"IpfsHash\":\"bafkreid56by3qspgyxvppq3ggj2x4tec3akpnlffbhangjzzfc3lfh66yq\",\"PinSize\":45,\"Timestamp\":\"2022-07-22T14:29:51.674Z\",\"isDuplicate\":true}'\n\n\n\n\n\npin\n\n pin (cred:str, cid:str, fn=None, pinataMetadata=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncred\nstr\n\nJWT key\n\n\ncid\nstr\n\nIPFS cid\n\n\nfn\nNoneType\nNone\nName of file\n\n\npinataMetadata\nNoneType\nNone\nAdd keys and values associated with IPFS CID\n\n\n\nPin files\n\nmetadata = {\"dApp\":\"Ethereum Name Service\",\n            \"token\":\"ENS\"\n           }\n\npin(creds,\"QmUUSHH2ycqciruPaRcptpUQDDeRiMV3G9PxA6KUupXTJV\",fn=\"ens_airdrop\",pinataMetadata=metadata)\n\n{\"id\":\"8cba5496-3cd4-4edb-92d7-023c0ce9c145\",\"ipfsHash\":\"QmUUSHH2ycqciruPaRcptpUQDDeRiMV3G9PxA6KUupXTJV\",\"status\":\"prechecking\",\"name\":\"ens_airdrop\"}\n\n\n\n\n\nunpin\n\n unpin (cred:str, cid:str)\n\n\n\n\n\nType\nDetails\n\n\n\n\ncred\nstr\nJWT Key\n\n\ncid\nstr\nIPFS CID\n\n\n\nUnpin IPFS CID\n\nunpin(creds,\"QmUUSHH2ycqciruPaRcptpUQDDeRiMV3G9PxA6KUupXTJV\")\n\n<Response [200]>\n\n\nIf IPFS CID isn’t pinned. A 500 error message will be returned\n\nunpin(creds,\"QmZnxARhJWsCbTxiAzoRhnxHgMtoEkNJNS8DGLCBEMvm4V\")\n\n<Response [500]>\n\n\n\n\n\nedit_metadata\n\n edit_metadata (cred:str, cid:str, name:str, metadata=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncred\nstr\n\nJWT Key\n\n\ncid\nstr\n\nIPFS CID\n\n\nname\nstr\n\nfilename\n\n\nmetadata\nNoneType\nNone\nAdd keys and values associated with IPFS CID\n\n\n\nEdit metadata of already pinned IPFS CID\n\nmetadata = {\"type\":\"Test\"}\n\nedit_metadata(creds,\"bafybeibcsnxt2sqzsvfg4lzg3iuxkfit37l3rz5gqmdxwsgec56qalmxie\",\"name\",metadata)\n\n<Response [200]>\n\n\nIf IPFS CID is not actively pinned. A 500 error message will be returned\n\nmetadata = {\"type\":\"Test\"}\n\nedit_metadata(creds,\"QmUUSHH2ycqciruPaRcptpUQDDeRiMV3G9PxA6KUupXTJV\",\"name\",metadata)\n\n<Response [500]>\n\n\n\n\n\nget_pinned_jobs\n\n get_pinned_jobs (cred:str, params=None)\n\n‘sort’ - Sort the results by the date added to the pinning queue (see value options below) ‘ASC’ - Sort by ascending dates ‘DESC’ - Sort by descending dates ‘status’ - Filter by the status of the job in the pinning queue (see potential statuses below) ‘prechecking’ - Pinata is running preliminary validations on your pin request. ‘searching’ - Pinata is actively searching for your content on the IPFS network. This may take some time if your content is isolated. ‘retrieving’ - Pinata has located your content and is now in the process of retrieving it. ‘expired’ - Pinata wasn’t able to find your content after a day of searching the IPFS network. Please make sure your content is hosted on the IPFS network before trying to pin again. ‘over_free_limit’ - Pinning this object would put you over the free tier limit. Please add a credit card to continue pinning content. ‘over_max_size’ - This object is too large of an item to pin. If you’re seeing this, please contact us for a more custom solution. ‘invalid_object’ - The object you’re attempting to pin isn’t readable by IPFS nodes. Please contact us if you receive this, as we’d like to better understand what you’re attempting to pin. ‘bad_host_node’ - You provided a host node that was either invalid or unreachable. Please make sure all provided host nodes are online and reachable. ‘ipfs_pin_hash’ - Retrieve the record for a specific IPFS hash ‘limit’ - Limit the amount of results returned per page of results (default is 5, and max is 1000) ‘offset’ - Provide the record offset for records being returned. This is how you retrieve records on additional pages (default is 0)\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncred\nstr\n\nJWT Key\n\n\nparams\nNoneType\nNone\nfiltering pinned jobs\n\n\n\nRetrieve pinned jobs. If there are pending pin jobs, they will be returned in json format.\n\nget_pinned_jobs(creds).json()\n\n{'count': 0, 'rows': []}\n\n\n\n\n\nget_pinned_files\n\n get_pinned_files (cred:str, params=None)\n\nQuery Parameters = params\nhashContains: (string) - Filter on alphanumeric characters inside of pin hashes. Hashes which do not include the characters passed in will not be returned. pinStart: (must be in ISO_8601 format) - Exclude pin records that were pinned before the passed in ‘pinStart’ datetime. pinEnd: (must be in ISO_8601 format) - Exclude pin records that were pinned after the passed in ‘pinEnd’ datetime. unpinStart: (must be in ISO_8601 format) - Exclude pin records that were unpinned before the passed in ‘unpinStart’ datetime. unpinEnd: (must be in ISO_8601 format) - Exclude pin records that were unpinned after the passed in ‘unpinEnd’ datetime. pinSizeMin: (integer) - The minimum byte size that pin record you’re looking for can have pinSizeMax: (integer) - The maximum byte size that pin record you’re looking for can have status: (string) - * Pass in ‘all’ for both pinned and unpinned records * Pass in ‘pinned’ for just pinned records (hashes that are currently pinned) * Pass in ‘unpinned’ for just unpinned records (previous hashes that are no longer being pinned on pinata) pageLimit: (integer) - This sets the amount of records that will be returned per API response. (Max 1000) pageOffset: (integer) - This tells the API how far to offset the record responses. For example, if there’s 30 records that match your query, and you passed in a pageLimit of 10, providing a pageOffset of 10 would return records 11-20.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncred\nstr\n\nJWT Key\n\n\nparams\nNoneType\nNone\nFilter returned pinned files\n\n\n\nRetrieve pinned files and use filtering arguments such as hashContains and status to filter by IPFS CID and get only the file that is pinned. Pinata keeps a log of all the times an IPFS CID is pinned and unpinned. Therefore, without the status filter multiple records would be returned.\n\nget_pinned_files(creds,params={\"hashContains\":\"bafybeibcsnxt2sqzsvfg4lzg3iuxkfit37l3rz5gqmdxwsgec56qalmxie\",\"status\":\"pinned\"}).json()\n\n{'count': 1,\n 'rows': [{'id': 'a1a93991-99bc-4c12-9553-2e689046cf65',\n   'ipfs_pin_hash': 'bafybeibcsnxt2sqzsvfg4lzg3iuxkfit37l3rz5gqmdxwsgec56qalmxie',\n   'size': 3975284,\n   'user_id': 'd28f20cf-dfe0-4c8f-9272-26b9c2d8a3cd',\n   'date_pinned': '2022-06-17T22:43:08.303Z',\n   'date_unpinned': None,\n   'metadata': {'name': 'name',\n    'keyvalues': {'Type': 'Test',\n     'type': 'Test',\n     'company': 'Pinata',\n     'filetype': 'json'}},\n   'regions': [{'regionId': 'NYC1',\n     'currentReplicationCount': 1,\n     'desiredReplicationCount': 1}]}]}\n\n\n\n\n\nget_datausage\n\n get_datausage (cred:str, params=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncred\nstr\n\nJWT Key\n\n\nparams\nNoneType\nNone\nFilter returned data usage statistics\n\n\n\nRetrieve data usage stats. The stats are in bytes.\n\nget_datausage(creds).json()\n\n{'pin_count': -223,\n 'pin_size_total': 349265187,\n 'pin_size_with_replications_total': 349265187}"
  }
]